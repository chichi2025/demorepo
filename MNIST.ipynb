{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chichi2025/demorepo/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTsB0y82aEoZ"
      },
      "outputs": [],
      "source": [
        "#Agenda:\n",
        "# Recap\n",
        "# MNIST data - SLP for MNIST data\n",
        "# MNIST data - Hand written digits data - black and white [grey scale]\n",
        "# 60000 training images of height 28 and width 28\n",
        "# 10000 test images of height 28 and width 28\n",
        "# Input: image with hand written image - 28 height and 28 width\n",
        "# Output: digit - [0,1,2,3,4,5,6,7,8, and 9]\n",
        "# Usecase: pincodes -"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Input and Output\n",
        "\n",
        "1a. Load the data [Completed]\n",
        "2. Data Processing [x data and y data] [Completed]\n",
        "3. Model design and development\n",
        "4. Model training\n",
        "5. Model Evaluation"
      ],
      "metadata": {
        "id": "PqPeesdbatXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Single layer Perceptron"
      ],
      "metadata": {
        "id": "3M_-HekurHYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf #\n",
        "from tensorflow import keras\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "print(X_train.shape)\n",
        "# (60000, 28, 28) - any idea what is this?\n",
        "# 60000 images of size 28*28\n",
        "print(X_test.shape)\n",
        "# (10000, 28, 28)\n",
        "# 10000 images of size 28*28\n",
        "# PIL - image to an array\n",
        "# array to image\n",
        "# 2x2 pixels - 4\n",
        "print(X_train[0]) # slicing by using index - referring to the first image of mnist data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Telo3Yg-dXIj",
        "outputId": "699e855f-f759-46b0-beb7-327c95e28de4",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data preprocessing"
      ],
      "metadata": {
        "id": "s-ZxCmqRiOme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshape = 784 # ANN - 3 dimensional array of data [NN - ANN [2 dim], CNN, RNN, RCNN, Transformers]\n",
        "X_train = X_train.reshape(60000, reshape)#(60000, 28, 28) is now reshaped to 60000, 784\n",
        "X_test = X_test.reshape(10000, reshape) #(10000, 28, 28) is now reshaped to 10000, 784\n",
        "print(X_train.shape) # why 784? why not 1000?\n",
        "print(X_test.shape)\n",
        "# Normalize the data\n",
        "X_train = X_train.astype('float32')/255.0 # 0-255 is scaled to 0 to 1\n",
        "X_test = X_test.astype('float32')/255.0\n",
        "print(X_train[0])\n",
        "# python - int, float, string, boolean\n",
        "# convert them to correct data type\n",
        "# either0 or 1 - int [240/255 - 0.94- 1]\n",
        "# float32 is a 32 bit number - lot of decimals\n",
        "# 0.19078401909104810987410874"
      ],
      "metadata": {
        "id": "_o5foMl2eLuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "91f308b1-c2da-4ac4-9028-356550931857"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
            " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215687\n",
            " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
            " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
            " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313726\n",
            " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.13725491 0.94509804\n",
            " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
            " 0.5882353  0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
            " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.5803922\n",
            " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058825\n",
            " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
            " 0.3137255  0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333336 0.99215686\n",
            " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape # (60000,) is (60000,1)\n",
        "# this is a classification problem\n",
        "# there is no ordinal relationship\n",
        "# ordinal - scale - [100 marks out of 100, 20 marks out of 100]\n",
        "# image of 9 -> if the model classifies it as 8 [lesser wrong]\n",
        "# image of 9 -> if the model classifies it as 1 [more wrong]\n",
        "# 71681\n",
        "# 21681\n",
        "# one hot encoding\n",
        "# y_values. A  B  C\n",
        "# A         1  0  0\n",
        "# B         0  1  0\n",
        "# A         1  0  0\n",
        "# C         0  0  1\n",
        "# how is B represented as one hot encoded vector\n",
        "# A- 1 0 0\n",
        "# one hot encoding\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, 10) # AS MANY UNIQUE CLASSES,\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, 10)\n",
        "print(Y_train.shape)\n",
        "print(Y_train[0])\n",
        "# why only 10? [not 20, 30, 50,100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmkV9FPwl296",
        "outputId": "5eabc80c-2319-4bfa-805d-206ff88c0f63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model design and development - SLP\n"
      ],
      "metadata": {
        "id": "2pHm4bootnlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense # fully connected\n",
        "from tensorflow.keras.models import Sequential # sequence of layers\n",
        "from tensorflow.keras.optimizers import SGD # stochastic gradient descent\n",
        "# forward propogation\n",
        "model = Sequential() # instantiating the class\n",
        "model.add(Dense(10,input_shape=(784,),activation='softmax')) # add a layer\n",
        "\n",
        "# back propogation\n",
        "model.compile(optimizer=SGD(learning_rate=0.002),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']) # accuracy after every iteration [epoch]\n",
        "\n",
        "training = model.fit(X_train,\n",
        "                     Y_train,\n",
        "                     batch_size= 100, # 1000 rows of data - 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000\n",
        "                     epochs=100,\n",
        "                     validation_data=(X_test, Y_test))\n",
        "\n",
        "# early stopping: monitor the values of training and validation\n",
        "# if validation is not improving further by 5 [ continous epochs]\n",
        "# stop the training\n",
        "\n",
        "# training accuracy > test accuracy : over train - overfitting\n",
        "# training accuracy < test accuracy : under train - underfitting\n",
        "# training accuracy ~ test accuracy by [0.5%]: under train - underfitting\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLB7VFscpOO6",
        "outputId": "3583d015-7ee6-4853-c1e4-9ab4e494a36e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2932 - loss: 2.0977 - val_accuracy: 0.6961 - val_loss: 1.4727\n",
            "Epoch 2/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7103 - loss: 1.3821 - val_accuracy: 0.7871 - val_loss: 1.0964\n",
            "Epoch 3/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7907 - loss: 1.0669 - val_accuracy: 0.8179 - val_loss: 0.9086\n",
            "Epoch 4/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8120 - loss: 0.9120 - val_accuracy: 0.8340 - val_loss: 0.7974\n",
            "Epoch 5/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.8030 - val_accuracy: 0.8461 - val_loss: 0.7237\n",
            "Epoch 6/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8384 - loss: 0.7376 - val_accuracy: 0.8534 - val_loss: 0.6709\n",
            "Epoch 7/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8440 - loss: 0.6910 - val_accuracy: 0.8597 - val_loss: 0.6310\n",
            "Epoch 8/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8510 - loss: 0.6509 - val_accuracy: 0.8637 - val_loss: 0.5997\n",
            "Epoch 9/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8560 - loss: 0.6187 - val_accuracy: 0.8665 - val_loss: 0.5745\n",
            "Epoch 10/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8577 - loss: 0.5953 - val_accuracy: 0.8691 - val_loss: 0.5535\n",
            "Epoch 11/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8616 - loss: 0.5729 - val_accuracy: 0.8728 - val_loss: 0.5357\n",
            "Epoch 12/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8647 - loss: 0.5569 - val_accuracy: 0.8742 - val_loss: 0.5205\n",
            "Epoch 13/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8671 - loss: 0.5394 - val_accuracy: 0.8767 - val_loss: 0.5072\n",
            "Epoch 14/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8692 - loss: 0.5268 - val_accuracy: 0.8786 - val_loss: 0.4956\n",
            "Epoch 15/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8693 - loss: 0.5188 - val_accuracy: 0.8804 - val_loss: 0.4852\n",
            "Epoch 16/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8741 - loss: 0.5060 - val_accuracy: 0.8828 - val_loss: 0.4760\n",
            "Epoch 17/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8751 - loss: 0.4950 - val_accuracy: 0.8841 - val_loss: 0.4678\n",
            "Epoch 18/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8746 - loss: 0.4977 - val_accuracy: 0.8848 - val_loss: 0.4601\n",
            "Epoch 19/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8741 - loss: 0.4893 - val_accuracy: 0.8860 - val_loss: 0.4533\n",
            "Epoch 20/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8776 - loss: 0.4744 - val_accuracy: 0.8871 - val_loss: 0.4469\n",
            "Epoch 21/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8792 - loss: 0.4699 - val_accuracy: 0.8882 - val_loss: 0.4410\n",
            "Epoch 22/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8822 - loss: 0.4600 - val_accuracy: 0.8887 - val_loss: 0.4357\n",
            "Epoch 23/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8802 - loss: 0.4597 - val_accuracy: 0.8898 - val_loss: 0.4307\n",
            "Epoch 24/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8807 - loss: 0.4558 - val_accuracy: 0.8904 - val_loss: 0.4260\n",
            "Epoch 25/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8841 - loss: 0.4474 - val_accuracy: 0.8913 - val_loss: 0.4216\n",
            "Epoch 26/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8832 - loss: 0.4477 - val_accuracy: 0.8916 - val_loss: 0.4176\n",
            "Epoch 27/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8844 - loss: 0.4385 - val_accuracy: 0.8923 - val_loss: 0.4137\n",
            "Epoch 28/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8840 - loss: 0.4373 - val_accuracy: 0.8927 - val_loss: 0.4102\n",
            "Epoch 29/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8860 - loss: 0.4304 - val_accuracy: 0.8933 - val_loss: 0.4067\n",
            "Epoch 30/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8872 - loss: 0.4271 - val_accuracy: 0.8940 - val_loss: 0.4035\n",
            "Epoch 31/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8886 - loss: 0.4230 - val_accuracy: 0.8941 - val_loss: 0.4005\n",
            "Epoch 32/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8871 - loss: 0.4221 - val_accuracy: 0.8942 - val_loss: 0.3975\n",
            "Epoch 33/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8882 - loss: 0.4201 - val_accuracy: 0.8942 - val_loss: 0.3947\n",
            "Epoch 34/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8882 - loss: 0.4175 - val_accuracy: 0.8951 - val_loss: 0.3921\n",
            "Epoch 35/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8893 - loss: 0.4140 - val_accuracy: 0.8954 - val_loss: 0.3897\n",
            "Epoch 36/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8919 - loss: 0.4082 - val_accuracy: 0.8960 - val_loss: 0.3873\n",
            "Epoch 37/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8894 - loss: 0.4107 - val_accuracy: 0.8967 - val_loss: 0.3851\n",
            "Epoch 38/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8931 - loss: 0.3981 - val_accuracy: 0.8967 - val_loss: 0.3829\n",
            "Epoch 39/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8928 - loss: 0.4035 - val_accuracy: 0.8973 - val_loss: 0.3808\n",
            "Epoch 40/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8938 - loss: 0.3973 - val_accuracy: 0.8981 - val_loss: 0.3788\n",
            "Epoch 41/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8912 - loss: 0.4038 - val_accuracy: 0.8989 - val_loss: 0.3769\n",
            "Epoch 42/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8929 - loss: 0.3972 - val_accuracy: 0.8997 - val_loss: 0.3750\n",
            "Epoch 43/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8939 - loss: 0.3962 - val_accuracy: 0.9004 - val_loss: 0.3732\n",
            "Epoch 44/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8927 - loss: 0.3984 - val_accuracy: 0.9002 - val_loss: 0.3714\n",
            "Epoch 45/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8937 - loss: 0.3923 - val_accuracy: 0.9007 - val_loss: 0.3698\n",
            "Epoch 46/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.3942 - val_accuracy: 0.9005 - val_loss: 0.3682\n",
            "Epoch 47/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8961 - loss: 0.3860 - val_accuracy: 0.9013 - val_loss: 0.3667\n",
            "Epoch 48/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8943 - loss: 0.3876 - val_accuracy: 0.9016 - val_loss: 0.3651\n",
            "Epoch 49/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8974 - loss: 0.3832 - val_accuracy: 0.9020 - val_loss: 0.3638\n",
            "Epoch 50/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8939 - loss: 0.3884 - val_accuracy: 0.9017 - val_loss: 0.3623\n",
            "Epoch 51/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8975 - loss: 0.3821 - val_accuracy: 0.9022 - val_loss: 0.3610\n",
            "Epoch 52/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8956 - loss: 0.3841 - val_accuracy: 0.9024 - val_loss: 0.3597\n",
            "Epoch 53/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8982 - loss: 0.3763 - val_accuracy: 0.9023 - val_loss: 0.3584\n",
            "Epoch 54/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8976 - loss: 0.3760 - val_accuracy: 0.9026 - val_loss: 0.3572\n",
            "Epoch 55/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8973 - loss: 0.3765 - val_accuracy: 0.9031 - val_loss: 0.3560\n",
            "Epoch 56/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8967 - loss: 0.3767 - val_accuracy: 0.9031 - val_loss: 0.3548\n",
            "Epoch 57/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8973 - loss: 0.3754 - val_accuracy: 0.9039 - val_loss: 0.3536\n",
            "Epoch 58/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8984 - loss: 0.3695 - val_accuracy: 0.9037 - val_loss: 0.3526\n",
            "Epoch 59/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8986 - loss: 0.3730 - val_accuracy: 0.9048 - val_loss: 0.3515\n",
            "Epoch 60/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8982 - loss: 0.3718 - val_accuracy: 0.9045 - val_loss: 0.3505\n",
            "Epoch 61/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9009 - loss: 0.3662 - val_accuracy: 0.9049 - val_loss: 0.3495\n",
            "Epoch 62/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8988 - loss: 0.3695 - val_accuracy: 0.9052 - val_loss: 0.3484\n",
            "Epoch 63/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9013 - loss: 0.3668 - val_accuracy: 0.9057 - val_loss: 0.3475\n",
            "Epoch 64/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9009 - loss: 0.3649 - val_accuracy: 0.9059 - val_loss: 0.3465\n",
            "Epoch 65/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9000 - loss: 0.3689 - val_accuracy: 0.9060 - val_loss: 0.3457\n",
            "Epoch 66/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9005 - loss: 0.3626 - val_accuracy: 0.9064 - val_loss: 0.3448\n",
            "Epoch 67/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9012 - loss: 0.3609 - val_accuracy: 0.9072 - val_loss: 0.3439\n",
            "Epoch 68/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9001 - loss: 0.3643 - val_accuracy: 0.9069 - val_loss: 0.3430\n",
            "Epoch 69/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9023 - loss: 0.3600 - val_accuracy: 0.9073 - val_loss: 0.3421\n",
            "Epoch 70/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9027 - loss: 0.3552 - val_accuracy: 0.9072 - val_loss: 0.3414\n",
            "Epoch 71/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9034 - loss: 0.3542 - val_accuracy: 0.9083 - val_loss: 0.3406\n",
            "Epoch 72/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9022 - loss: 0.3556 - val_accuracy: 0.9082 - val_loss: 0.3397\n",
            "Epoch 73/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9014 - loss: 0.3605 - val_accuracy: 0.9088 - val_loss: 0.3391\n",
            "Epoch 74/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9021 - loss: 0.3567 - val_accuracy: 0.9092 - val_loss: 0.3383\n",
            "Epoch 75/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9028 - loss: 0.3542 - val_accuracy: 0.9095 - val_loss: 0.3376\n",
            "Epoch 76/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9007 - loss: 0.3573 - val_accuracy: 0.9095 - val_loss: 0.3369\n",
            "Epoch 77/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9044 - loss: 0.3492 - val_accuracy: 0.9094 - val_loss: 0.3362\n",
            "Epoch 78/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9030 - loss: 0.3499 - val_accuracy: 0.9097 - val_loss: 0.3355\n",
            "Epoch 79/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9026 - loss: 0.3542 - val_accuracy: 0.9097 - val_loss: 0.3348\n",
            "Epoch 80/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9026 - loss: 0.3520 - val_accuracy: 0.9100 - val_loss: 0.3342\n",
            "Epoch 81/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9040 - loss: 0.3480 - val_accuracy: 0.9102 - val_loss: 0.3336\n",
            "Epoch 82/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9028 - loss: 0.3503 - val_accuracy: 0.9102 - val_loss: 0.3329\n",
            "Epoch 83/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9040 - loss: 0.3513 - val_accuracy: 0.9105 - val_loss: 0.3323\n",
            "Epoch 84/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9043 - loss: 0.3466 - val_accuracy: 0.9102 - val_loss: 0.3316\n",
            "Epoch 85/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9034 - loss: 0.3515 - val_accuracy: 0.9108 - val_loss: 0.3310\n",
            "Epoch 86/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9030 - loss: 0.3503 - val_accuracy: 0.9105 - val_loss: 0.3305\n",
            "Epoch 87/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9040 - loss: 0.3471 - val_accuracy: 0.9107 - val_loss: 0.3299\n",
            "Epoch 88/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9049 - loss: 0.3445 - val_accuracy: 0.9104 - val_loss: 0.3293\n",
            "Epoch 89/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9032 - loss: 0.3531 - val_accuracy: 0.9105 - val_loss: 0.3288\n",
            "Epoch 90/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9048 - loss: 0.3426 - val_accuracy: 0.9108 - val_loss: 0.3282\n",
            "Epoch 91/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9039 - loss: 0.3410 - val_accuracy: 0.9106 - val_loss: 0.3277\n",
            "Epoch 92/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9029 - loss: 0.3473 - val_accuracy: 0.9108 - val_loss: 0.3272\n",
            "Epoch 93/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9058 - loss: 0.3424 - val_accuracy: 0.9107 - val_loss: 0.3267\n",
            "Epoch 94/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9044 - loss: 0.3425 - val_accuracy: 0.9107 - val_loss: 0.3262\n",
            "Epoch 95/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9072 - loss: 0.3370 - val_accuracy: 0.9109 - val_loss: 0.3257\n",
            "Epoch 96/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9055 - loss: 0.3398 - val_accuracy: 0.9112 - val_loss: 0.3252\n",
            "Epoch 97/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9051 - loss: 0.3420 - val_accuracy: 0.9112 - val_loss: 0.3247\n",
            "Epoch 98/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9055 - loss: 0.3389 - val_accuracy: 0.9117 - val_loss: 0.3242\n",
            "Epoch 99/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9041 - loss: 0.3392 - val_accuracy: 0.9112 - val_loss: 0.3238\n",
            "Epoch 100/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9047 - loss: 0.3406 - val_accuracy: 0.9117 - val_loss: 0.3233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = model.fit(X_train,\n",
        "                     Y_train,\n",
        "                     batch_size= 100, # 1000 rows of data - 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000\n",
        "                     epochs=100,\n",
        "                     validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpOOqp4x5JyF",
        "outputId": "29a3f79a-c3c0-4cf9-bd90-7e7246d304c6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9047 - loss: 0.3440 - val_accuracy: 0.9118 - val_loss: 0.3228\n",
            "Epoch 2/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9069 - loss: 0.3371 - val_accuracy: 0.9121 - val_loss: 0.3224\n",
            "Epoch 3/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9047 - loss: 0.3401 - val_accuracy: 0.9122 - val_loss: 0.3220\n",
            "Epoch 4/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9050 - loss: 0.3416 - val_accuracy: 0.9123 - val_loss: 0.3216\n",
            "Epoch 5/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9061 - loss: 0.3375 - val_accuracy: 0.9123 - val_loss: 0.3211\n",
            "Epoch 6/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9075 - loss: 0.3342 - val_accuracy: 0.9123 - val_loss: 0.3206\n",
            "Epoch 7/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9070 - loss: 0.3340 - val_accuracy: 0.9127 - val_loss: 0.3203\n",
            "Epoch 8/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9086 - loss: 0.3305 - val_accuracy: 0.9128 - val_loss: 0.3199\n",
            "Epoch 9/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9067 - loss: 0.3342 - val_accuracy: 0.9129 - val_loss: 0.3195\n",
            "Epoch 10/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9069 - loss: 0.3355 - val_accuracy: 0.9129 - val_loss: 0.3191\n",
            "Epoch 11/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9077 - loss: 0.3354 - val_accuracy: 0.9132 - val_loss: 0.3188\n",
            "Epoch 12/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9058 - loss: 0.3349 - val_accuracy: 0.9132 - val_loss: 0.3183\n",
            "Epoch 13/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9084 - loss: 0.3310 - val_accuracy: 0.9135 - val_loss: 0.3179\n",
            "Epoch 14/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9081 - loss: 0.3339 - val_accuracy: 0.9135 - val_loss: 0.3176\n",
            "Epoch 15/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9073 - loss: 0.3340 - val_accuracy: 0.9135 - val_loss: 0.3172\n",
            "Epoch 16/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9075 - loss: 0.3349 - val_accuracy: 0.9138 - val_loss: 0.3169\n",
            "Epoch 17/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9096 - loss: 0.3305 - val_accuracy: 0.9143 - val_loss: 0.3165\n",
            "Epoch 18/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9071 - loss: 0.3349 - val_accuracy: 0.9145 - val_loss: 0.3161\n",
            "Epoch 19/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9063 - loss: 0.3364 - val_accuracy: 0.9145 - val_loss: 0.3158\n",
            "Epoch 20/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9045 - loss: 0.3361 - val_accuracy: 0.9144 - val_loss: 0.3154\n",
            "Epoch 21/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9079 - loss: 0.3326 - val_accuracy: 0.9144 - val_loss: 0.3151\n",
            "Epoch 22/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9086 - loss: 0.3286 - val_accuracy: 0.9145 - val_loss: 0.3149\n",
            "Epoch 23/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9069 - loss: 0.3313 - val_accuracy: 0.9147 - val_loss: 0.3145\n",
            "Epoch 24/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9092 - loss: 0.3253 - val_accuracy: 0.9145 - val_loss: 0.3142\n",
            "Epoch 25/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9085 - loss: 0.3254 - val_accuracy: 0.9146 - val_loss: 0.3138\n",
            "Epoch 26/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9076 - loss: 0.3309 - val_accuracy: 0.9147 - val_loss: 0.3135\n",
            "Epoch 27/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9073 - loss: 0.3324 - val_accuracy: 0.9148 - val_loss: 0.3132\n",
            "Epoch 28/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9085 - loss: 0.3245 - val_accuracy: 0.9151 - val_loss: 0.3129\n",
            "Epoch 29/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9089 - loss: 0.3245 - val_accuracy: 0.9149 - val_loss: 0.3127\n",
            "Epoch 30/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9078 - loss: 0.3282 - val_accuracy: 0.9154 - val_loss: 0.3123\n",
            "Epoch 31/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9102 - loss: 0.3212 - val_accuracy: 0.9154 - val_loss: 0.3120\n",
            "Epoch 32/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9107 - loss: 0.3224 - val_accuracy: 0.9154 - val_loss: 0.3117\n",
            "Epoch 33/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9090 - loss: 0.3257 - val_accuracy: 0.9153 - val_loss: 0.3114\n",
            "Epoch 34/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9094 - loss: 0.3243 - val_accuracy: 0.9153 - val_loss: 0.3111\n",
            "Epoch 35/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9109 - loss: 0.3243 - val_accuracy: 0.9152 - val_loss: 0.3109\n",
            "Epoch 36/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9108 - loss: 0.3202 - val_accuracy: 0.9156 - val_loss: 0.3106\n",
            "Epoch 37/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9092 - loss: 0.3273 - val_accuracy: 0.9156 - val_loss: 0.3103\n",
            "Epoch 38/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9113 - loss: 0.3208 - val_accuracy: 0.9156 - val_loss: 0.3100\n",
            "Epoch 39/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9095 - loss: 0.3239 - val_accuracy: 0.9159 - val_loss: 0.3097\n",
            "Epoch 40/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9112 - loss: 0.3203 - val_accuracy: 0.9162 - val_loss: 0.3095\n",
            "Epoch 41/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9092 - loss: 0.3231 - val_accuracy: 0.9159 - val_loss: 0.3092\n",
            "Epoch 42/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9104 - loss: 0.3221 - val_accuracy: 0.9159 - val_loss: 0.3090\n",
            "Epoch 43/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9099 - loss: 0.3228 - val_accuracy: 0.9159 - val_loss: 0.3087\n",
            "Epoch 44/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9104 - loss: 0.3236 - val_accuracy: 0.9161 - val_loss: 0.3085\n",
            "Epoch 45/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9117 - loss: 0.3185 - val_accuracy: 0.9161 - val_loss: 0.3082\n",
            "Epoch 46/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9110 - loss: 0.3202 - val_accuracy: 0.9161 - val_loss: 0.3080\n",
            "Epoch 47/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9125 - loss: 0.3158 - val_accuracy: 0.9160 - val_loss: 0.3077\n",
            "Epoch 48/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9091 - loss: 0.3241 - val_accuracy: 0.9160 - val_loss: 0.3074\n",
            "Epoch 49/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9099 - loss: 0.3271 - val_accuracy: 0.9162 - val_loss: 0.3072\n",
            "Epoch 50/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9085 - loss: 0.3275 - val_accuracy: 0.9162 - val_loss: 0.3070\n",
            "Epoch 51/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9125 - loss: 0.3171 - val_accuracy: 0.9162 - val_loss: 0.3068\n",
            "Epoch 52/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9127 - loss: 0.3178 - val_accuracy: 0.9163 - val_loss: 0.3065\n",
            "Epoch 53/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9068 - loss: 0.3279 - val_accuracy: 0.9162 - val_loss: 0.3063\n",
            "Epoch 54/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9107 - loss: 0.3195 - val_accuracy: 0.9161 - val_loss: 0.3061\n",
            "Epoch 55/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9112 - loss: 0.3156 - val_accuracy: 0.9167 - val_loss: 0.3058\n",
            "Epoch 56/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9098 - loss: 0.3192 - val_accuracy: 0.9167 - val_loss: 0.3056\n",
            "Epoch 57/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9132 - loss: 0.3117 - val_accuracy: 0.9164 - val_loss: 0.3054\n",
            "Epoch 58/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9124 - loss: 0.3179 - val_accuracy: 0.9165 - val_loss: 0.3052\n",
            "Epoch 59/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9116 - loss: 0.3185 - val_accuracy: 0.9165 - val_loss: 0.3050\n",
            "Epoch 60/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9114 - loss: 0.3172 - val_accuracy: 0.9166 - val_loss: 0.3048\n",
            "Epoch 61/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9127 - loss: 0.3146 - val_accuracy: 0.9170 - val_loss: 0.3045\n",
            "Epoch 62/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9103 - loss: 0.3223 - val_accuracy: 0.9170 - val_loss: 0.3043\n",
            "Epoch 63/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9102 - loss: 0.3240 - val_accuracy: 0.9171 - val_loss: 0.3042\n",
            "Epoch 64/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9115 - loss: 0.3200 - val_accuracy: 0.9172 - val_loss: 0.3040\n",
            "Epoch 65/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9103 - loss: 0.3167 - val_accuracy: 0.9172 - val_loss: 0.3037\n",
            "Epoch 66/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9143 - loss: 0.3105 - val_accuracy: 0.9174 - val_loss: 0.3035\n",
            "Epoch 67/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9128 - loss: 0.3162 - val_accuracy: 0.9170 - val_loss: 0.3033\n",
            "Epoch 68/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9111 - loss: 0.3187 - val_accuracy: 0.9172 - val_loss: 0.3031\n",
            "Epoch 69/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9104 - loss: 0.3190 - val_accuracy: 0.9171 - val_loss: 0.3029\n",
            "Epoch 70/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9126 - loss: 0.3144 - val_accuracy: 0.9171 - val_loss: 0.3027\n",
            "Epoch 71/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9127 - loss: 0.3187 - val_accuracy: 0.9173 - val_loss: 0.3025\n",
            "Epoch 72/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9119 - loss: 0.3124 - val_accuracy: 0.9171 - val_loss: 0.3023\n",
            "Epoch 73/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9140 - loss: 0.3100 - val_accuracy: 0.9172 - val_loss: 0.3022\n",
            "Epoch 74/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9122 - loss: 0.3148 - val_accuracy: 0.9173 - val_loss: 0.3020\n",
            "Epoch 75/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9112 - loss: 0.3169 - val_accuracy: 0.9173 - val_loss: 0.3019\n",
            "Epoch 76/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9119 - loss: 0.3169 - val_accuracy: 0.9171 - val_loss: 0.3016\n",
            "Epoch 77/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9146 - loss: 0.3104 - val_accuracy: 0.9171 - val_loss: 0.3014\n",
            "Epoch 78/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9130 - loss: 0.3105 - val_accuracy: 0.9176 - val_loss: 0.3013\n",
            "Epoch 79/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9131 - loss: 0.3119 - val_accuracy: 0.9176 - val_loss: 0.3011\n",
            "Epoch 80/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9124 - loss: 0.3166 - val_accuracy: 0.9174 - val_loss: 0.3009\n",
            "Epoch 81/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9146 - loss: 0.3071 - val_accuracy: 0.9175 - val_loss: 0.3008\n",
            "Epoch 82/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9145 - loss: 0.3111 - val_accuracy: 0.9176 - val_loss: 0.3005\n",
            "Epoch 83/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9118 - loss: 0.3147 - val_accuracy: 0.9178 - val_loss: 0.3004\n",
            "Epoch 84/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9123 - loss: 0.3158 - val_accuracy: 0.9177 - val_loss: 0.3002\n",
            "Epoch 85/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9146 - loss: 0.3110 - val_accuracy: 0.9176 - val_loss: 0.3000\n",
            "Epoch 86/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9116 - loss: 0.3115 - val_accuracy: 0.9175 - val_loss: 0.2999\n",
            "Epoch 87/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9155 - loss: 0.3086 - val_accuracy: 0.9178 - val_loss: 0.2997\n",
            "Epoch 88/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9151 - loss: 0.3071 - val_accuracy: 0.9178 - val_loss: 0.2995\n",
            "Epoch 89/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9136 - loss: 0.3164 - val_accuracy: 0.9177 - val_loss: 0.2994\n",
            "Epoch 90/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.3143 - val_accuracy: 0.9177 - val_loss: 0.2992\n",
            "Epoch 91/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.3083 - val_accuracy: 0.9179 - val_loss: 0.2991\n",
            "Epoch 92/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9131 - loss: 0.3121 - val_accuracy: 0.9179 - val_loss: 0.2989\n",
            "Epoch 93/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9137 - loss: 0.3153 - val_accuracy: 0.9182 - val_loss: 0.2988\n",
            "Epoch 94/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9144 - loss: 0.3124 - val_accuracy: 0.9180 - val_loss: 0.2986\n",
            "Epoch 95/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9131 - loss: 0.3078 - val_accuracy: 0.9180 - val_loss: 0.2985\n",
            "Epoch 96/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9156 - loss: 0.3106 - val_accuracy: 0.9177 - val_loss: 0.2983\n",
            "Epoch 97/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9143 - loss: 0.3086 - val_accuracy: 0.9180 - val_loss: 0.2982\n",
            "Epoch 98/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9137 - loss: 0.3105 - val_accuracy: 0.9178 - val_loss: 0.2980\n",
            "Epoch 99/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9170 - loss: 0.3036 - val_accuracy: 0.9179 - val_loss: 0.2979\n",
            "Epoch 100/100\n",
            "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9127 - loss: 0.3124 - val_accuracy: 0.9175 - val_loss: 0.2977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo7lziSk3a_4",
        "outputId": "97fde007-d9f0-4a65-d9b8-15785e6bb595",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Variable path=sequential/dense/kernel, shape=(784, 10), dtype=float32, value=[[-0.03297242  0.00607815  0.01803157 ...  0.00669045 -0.0070243\n",
              "    0.06464203]\n",
              "  [ 0.07545767 -0.05501254  0.08638591 ...  0.02005851 -0.01236966\n",
              "   -0.00197446]\n",
              "  [-0.07301164  0.03778291 -0.07468501 ...  0.01844418  0.01858308\n",
              "    0.06726513]\n",
              "  ...\n",
              "  [-0.01834412 -0.00864121 -0.05208781 ...  0.03561681  0.05152345\n",
              "    0.07959116]\n",
              "  [-0.04324067  0.00783014  0.05202154 ...  0.07439784 -0.04650666\n",
              "   -0.01530391]\n",
              "  [-0.03041663 -0.04090944  0.07705941 ... -0.06025933 -0.02771736\n",
              "   -0.04685995]]>,\n",
              " <Variable path=sequential/dense/bias, shape=(10,), dtype=float32, value=[-0.40799713  0.35634246  0.11170088 -0.29223707  0.03219809  1.3576678\n",
              "  -0.08969318  0.6854709  -1.4940183  -0.2594313 ]>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A(X.W+b) = yp\n",
        "## Forward propogation\n",
        "# AS MANY FEATURES, SO MANY NODES IN YOUR INPUT LAYER\n",
        "# AS MANY UNIQUE CLASSES, SO MANY NODES IN YOUR OUTPUT LAYER\n",
        "# EVERY NODE IN ONE LAYER IS CONNECTED TO EVERY OTHER NODE IN THE\n",
        "# IMMEDIATE NEXT LAYER - FULLY CONNECTED\n",
        "\n",
        "# WHAT IS THE SHAPE OF X DATA\n",
        "# WHAT IS THE SHAPE OF Y DATA\n",
        "# IF A is a array of shape (m,n) and B is a array of shape (n,o)\n",
        "# than A.B is a matrix of shape (m,o)"
      ],
      "metadata": {
        "id": "JDx2TBXa1OnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 mins - class\n",
        "# 20 mins - practice\n",
        "#"
      ],
      "metadata": {
        "id": "GyjVHZJKm8IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. MultiLayer Perceptron [MLP]\n",
        "\n",
        "functional composition -\n",
        "sequential composition - output of last layer-> input to the next layer\n",
        "\n",
        "(None,784).(784,16) = (None,16)\n",
        "\n",
        "for all images - alex net suggests cnn->relu - accuracy\n",
        "\n",
        "number of nodes in the hidden layer, number of layers -\n",
        "\n",
        "(None,16).(16,10) = (None,10)\n"
      ],
      "metadata": {
        "id": "e4shcy7LmGdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense # fully connected\n",
        "from tensorflow.keras.models import Sequential # sequence of layers\n",
        "from tensorflow.keras.optimizers import SGD # stochastic gradient descent\n",
        "\n",
        "# forward propogation\n",
        "model = Sequential() # instantiating the class\n",
        "model.add(Dense(16,input_shape=(784,),activation='sigmoid')) # add a layer\n",
        "# Chirag - 'Sigmoid'  instead of 'relu'\n",
        "model.add(Dense(10,activation='softmax')) # add a layer\n",
        "\n",
        "\n",
        "# back propogation\n",
        "model.compile(optimizer=SGD(learning_rate=0.002),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']) # accuracy after every iteration [epoch]\n",
        "\n",
        "training = model.fit(X_train,\n",
        "                     Y_train,\n",
        "                     batch_size= 100, # 1000 rows of data - 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000\n",
        "                     epochs=100,\n",
        "                     validation_data=(X_test, Y_test))\n"
      ],
      "metadata": {
        "id": "O72Qac_MmKTo",
        "outputId": "7b8908a8-d825-416f-c535-9bc41577dcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not interpret activation function identifier: Sigmoid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-92414b9c89b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# forward propogation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# instantiating the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Chirag - 'Sigmoid'  instead of 'relu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/activations/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;34mf\"Could not interpret activation function identifier: {identifier}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not interpret activation function identifier: Sigmoid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# understanding the floats\n",
        "# import numpy as np\n",
        "# a = np.array([0.19078401909104810987410874], dtype=np.float16)\n",
        "# print(\"16bit\", a[0])\n",
        "# a1 = np.array([0.19078401909104810987410874], dtype=np.float32)\n",
        "# print(\"32bit\", a1[0])\n",
        "# a2 = np.array([0.19078401909104810987410874], dtype=np.float64)\n",
        "# print(\"64bit\", a2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMwljgmfe-t5",
        "outputId": "7b3917fb-be46-4703-e9c6-d1a504556ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16bit 0.1908\n",
            "32bit 0.19078402\n",
            "64bit 0.1907840190910481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "uENlT2zVtXQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}